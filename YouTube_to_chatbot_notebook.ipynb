{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmethalm/youtube-to-chatbot/blob/main/YouTube_to_chatbot_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWwrfbbVGOA"
      },
      "source": [
        "#### ðŸ‘‰ [Original launch announcement](https://twitter.com/ehalm_/status/1660914850107883520?s=20) ðŸš€\n",
        "\n",
        "# YouTube-to-Chatbot\n",
        "\n",
        "Train a chatbot on an entire YouTube channel.\n",
        "\n",
        "This notebook uses YouTube, OpenAI, Langchain, and Pinecone to build a conversational agent trained to mimic the content, knowledge, and tone of a YouTube channel.\n",
        "\n",
        "Simply add in the YouTube ID of the channel you'd like to clone, add your API keys (OpenAI, YouTube, Pinecone), and run each step of the notebook. My hope is that this project unlocks more values for creators & helps grow communities.\n",
        "\n",
        "## To get early access to new features, follow [@ehalm_](https://twitter.com/ehalm_) on Twitter. If you're a creator looking for a custom model or a developer wanting to contribute, DM me :)\n",
        "\n",
        
        "Emmet âœŒ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "2FIzZ73bR5yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pva9ehKXUpU2"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    openai==0.27.7 \\\n",
        "    pinecone-client[grpc]==2.2.1 \\\n",
        "    langchain==0.0.162 \\\n",
        "    tiktoken==0.4.0 \\\n",
        "    datasets==2.12.0 \\\n",
        "    youtube_transcript_api "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape an entire YouTube channel into one .txt file"
      ],
      "metadata": {
        "id": "YZ7U_sF7VlS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"\" #@param {type:\"string\"}\n",
        "channel_id = \"\" #@param {type:\"string\"} #Your channel ID here (about --> inspect source --> UC...)\n",
        "import os\n",
        "import googleapiclient.discovery\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "def get_channel_videos(channel_id, api_key):\n",
        "    youtube = googleapiclient.discovery.build(\n",
        "        \"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    video_ids = []\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.search().list(\n",
        "            part=\"snippet\",\n",
        "            channelId=channel_id,\n",
        "            maxResults=50,  # Fetch 50 videos at a time\n",
        "            pageToken=page_token  # Add pagination\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        video_ids += [item['id']['videoId'] for item in response['items'] if item['id']['kind'] == 'youtube#video']\n",
        "        \n",
        "        # Check if there are more videos to fetch\n",
        "        if 'nextPageToken' in response:\n",
        "            page_token = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return video_ids\n",
        "\n",
        "def get_transcripts(video_ids):\n",
        "    transcripts = []\n",
        "    for video_id in video_ids:\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            transcripts.append(transcript)\n",
        "        except:\n",
        "            print(f\"An error occurred for video: {video_id}\")\n",
        "    return transcripts\n",
        "\n",
        "def write_to_file(transcripts):\n",
        "    with open('YouTube.txt', 'w') as f:\n",
        "        for transcript in transcripts:\n",
        "            for item in transcript:\n",
        "                f.write(item['text'] + '\\n')\n",
        "\n",
        "def main(api_key, channel_id):\n",
        "    video_ids = get_channel_videos(channel_id, api_key)\n",
        "    transcripts = get_transcripts(video_ids)\n",
        "    write_to_file(transcripts)\n",
        "\n",
        "main(api_key, channel_id)"
      ],
      "metadata": {
        "id": "FqE3b7L6VtR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run our text file through infiniteGPT to clean its grammar and punctuation."
      ],
      "metadata": {
        "id": "ZukPFl65V-lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_OPENAI_API_KEY = \"\" #@param {type:\"string\"}\n",
        "import openai\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tiktoken\n",
        "\n",
        "    # Add your own OpenAI API key\n",
        "\n",
        "openai.api_key = YOUR_OPENAI_API_KEY\n",
        "\n",
        "def load_text(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read()\n",
        "\n",
        "def save_to_file(responses, output_file):\n",
        "    with open(output_file, 'w') as file:\n",
        "        for response in responses:\n",
        "            file.write(response + '\\n')\n",
        "\n",
        "    # Change your OpenAI chat model accordingly\n",
        "\n",
        "def call_openai_api(chunk):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Clean the following transcripts of all gramatical mistakes, misplaced words, and identify the speakers.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{chunk}\"},\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return response.choices[0]['message']['content'].strip()\n",
        "\n",
        "def split_into_chunks(text, tokens=500):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "    words = encoding.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), tokens):\n",
        "        chunks.append(' '.join(encoding.decode(words[i:i + tokens])))\n",
        "    return chunks   \n",
        "\n",
        "import time\n",
        "\n",
        "def process_chunks(input_file, output_file, delay=1/58):  # delay in seconds (if you hit a rate limit error)\n",
        "    text = load_text(input_file)\n",
        "    chunks = split_into_chunks(text)\n",
        "    \n",
        "    responses = []\n",
        "    for chunk in chunks:\n",
        "        responses.append(call_openai_api(chunk))\n",
        "        time.sleep(delay)\n",
        "\n",
        "    save_to_file(responses, output_file)\n",
        "\n",
        "    # Specify your input and output files\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"YouTube.txt\"\n",
        "    output_file = \"clean_transcript.txt\"\n",
        "    process_chunks(input_file, output_file)\n",
        "\n",
        "    # Can take up to a few minutes to run depending on the size of your data input"
      ],
      "metadata": {
        "id": "mrGqeGjUWJW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTgrOQziXUto"
      },
      "source": [
        "## Building the Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNyRsz0ZXXaq"
      },
      "source": [
        "We start by constructing our knowledge base. Make sure to input the correct .txt file. For most users, this will be your cleaned_transcript.txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laSDMjqQXuj-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the .txt file into Python\n",
        "with open('cleaned_transcript.txt', 'r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "\n",
        "# Now lines is a list where each element is a line from the .txt file\n",
        "\n",
        "# Group the lines into chunks of 5\n",
        "chunks = [' '.join(lines[i:i+5]) for i in range(0, len(lines), 5)]\n",
        "\n",
        "# Convert list of chunks into a DataFrame\n",
        "data = pd.DataFrame(chunks, columns=['context'])\n",
        "\n",
        "# Add an index column and a name column\n",
        "data['name'] = 'youtube'\n",
        "\n",
        "# Remove duplicates (if any)\n",
        "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_Pt7N6Zg2X"
      },
      "source": [
        "### Initialize the Embedding Model and Vector DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGoS84KYZnSK"
      },
      "source": [
        "We'll be using OpenAI's `text-embedding-ada-002` model initialize via LangChain and the Pinecone vector DB. We start by initializing the embedding model, for this we need an [OpenAI API key](https://platform.openai.com/).\n",
        "\n",
        "*(Note that OpenAI is a paid service and so running the remainder of this notebook will cost a few dimes)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U57x2_87YSpb"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")  # platform.openai.com\n",
        "model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQTfOTR6aBRS"
      },
      "source": [
        "Next we initialize the vector database. For this we need a [free API key](https://app.pinecone.io/), then we create the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3wrG-9yaJel"
      },
      "outputs": [],
      "source": [
        " import pinecone\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "\n",
        "YOUR_API_KEY = '' #@param {type:\"string\"}\n",
        "# find ENV (cloud region) next to API key in console\n",
        "YOUR_ENV = '' #@param {type:\"string\"}\n",
        "\n",
        "index_name = 'youtube-chatbot-agent'\n",
        "pinecone.init(\n",
        "    api_key=YOUR_API_KEY,\n",
        "    environment=YOUR_ENV\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='dotproduct',\n",
        "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiSWrAQ5aRco"
      },
      "source": [
        "Then connect to the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfsfuFmqaS4G"
      },
      "outputs": [],
      "source": [
        "index = pinecone.GRPCIndex(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD5IGOoLaVx7"
      },
      "source": [
        "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
        "\n",
        "## Indexing\n",
        "\n",
        "We can perform the indexing task using the Pinecone python client directly. We will do this in batches of `100` or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9eea814462bb48f2ac468e649311019b",
            "53bfc84e8a4c43f98095e6dcc543f85a",
            "4ea0d00cccae46ebbe5043d5910667e9",
            "ca5c4ed971e94b4aa0dd8b8abc61363a",
            "985bf8b6b856453eb49218880723344b",
            "0c7f76f9ecce49baa310f1acff5450a9",
            "ac9aff5b622848f99528d8b1ef39d1f7",
            "5604993df40848f49b4f39c2f9e3855c",
            "8c169a1c70854b07bbcbe5fcbb508429",
            "000f3fb5bce64e5980d53a64ec814d73",
            "ee17cdf174f440459ab0dfdd20e109b3"
          ]
        },
        "id": "AhDcbRGTaWPi",
        "outputId": "eec92834-8ceb-486e-f666-bccd3a1e218d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eea814462bb48f2ac468e649311019b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "# Reset index and ensure 'index' column is added\n",
        "data = data.reset_index(drop=True)\n",
        "data = data.reset_index()\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    # get end of batch\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    batch = data.iloc[i:i_end]\n",
        "    \n",
        "    # first get metadata fields for this record\n",
        "    metadatas = [{\n",
        "      'text': record[0],  # 'text' will contain the same data as 'context'\n",
        "      'name': record[1]\n",
        "    } for record in batch.itertuples(index=False)]\n",
        "    \n",
        "    # get the list of contexts / documents\n",
        "    documents = batch['context'].tolist()\n",
        "    \n",
        "    # create document embeddings\n",
        "    embeds = embed.embed_documents(documents)\n",
        "    \n",
        "    # get IDs and convert them to strings\n",
        "    ids = batch['index'].astype(str).tolist()\n",
        "    \n",
        "    # add everything to pinecone\n",
        "    index.upsert(vectors=list(zip(ids, embeds, metadatas)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDUnLdy1b7G1"
      },
      "source": [
        "We've indexed everything, now we can check the number of vectors in our index like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiccGZKAb_Qo"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-3oolT5cCR8"
      },
      "source": [
        "## Creating a Vector Store and Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZ12U06cCH5"
      },
      "source": [
        "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MBJ477-cFNw"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K3xRthWcXzW"
      },
      "source": [
        "As in previous examples, we can use the `similarity_search` method to do a pure semantic search (without the generation component)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uITMZtzschJF"
      },
      "outputs": [],
      "source": [
        "query = \"how to make viral videos?\"\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zGF6YsgczqT"
      },
      "source": [
        "Looks like we're getting good results. Let's take a look at how we can begin integrating this into a conversational agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFsIOm73dcOI"
      },
      "source": [
        "## Initializing the Conversational Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMv6TXWkdfNR"
      },
      "source": [
        "Our conversational agent needs a Chat LLM, conversational memory, and a `RetrievalQA` chain to initialize. We create these using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMRs9Klic5-Y"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# chat completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "# conversational memory\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")\n",
        "# retrieval qa chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ySfWyZLdboX"
      },
      "source": [
        "Using these we can generate an answer using the `run` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaYSq0V-dxHw"
      },
      "outputs": [],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtSXR5RXdyU0"
      },
      "source": [
        "But this isn't yet ready for our conversational agent. For that we need to convert this retrieval chain into a tool. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwCYrS4duqBW"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name='Knowledge Base',\n",
        "        func=qa.run,\n",
        "        description=(\n",
        "            'use this tool when answering specific queries to get '\n",
        "            'more information and stories on the topic'\n",
        "        )\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXi_0ipTvM_l"
      },
      "source": [
        "Now we can initialize the agent like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaKTzPUEvOoy"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent='chat-conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        "    memory=conversational_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbXl-AzVvszB"
      },
      "source": [
        "With that our retrieval augmented conversational agent is ready and we can begin using it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "id": "Qjh1aZ_ppzPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg = \"\"\"You are [NAME], a [description] & content creator. You are an expert in [description]. Answer the user's questions with [desired tone].\n",
        "\"\"\"\n",
        "\n",
        "prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "agent.agent.llm_chain.prompt = prompt"
      ],
      "metadata": {
        "id": "7G_Djcu-qBKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "id": "KRgxFTkuqUQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlxUBWKcvzeP"
      },
      "source": [
        "### Using the Conversational Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZapCP4Pv2kz"
      },
      "source": [
        "To make queries we simply call the `agent` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJoAhy76vzAB"
      },
      "outputs": [],
      "source": [
        "agent(\"Tell me about yourself\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWivmw9F3bCw"
      },
      "source": [
        "We're also able to ask questions that refer to previous interactions in the conversation and the agent is able to refer to the conversation history to as a source of information.\n",
        "\n",
        "Once finished, delete the Pinecone index to save resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa1whr8V3Wfm"
      },
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykg5TYA033yR"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9eea814462bb48f2ac468e649311019b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53bfc84e8a4c43f98095e6dcc543f85a",
              "IPY_MODEL_4ea0d00cccae46ebbe5043d5910667e9",
              "IPY_MODEL_ca5c4ed971e94b4aa0dd8b8abc61363a"
            ],
            "layout": "IPY_MODEL_985bf8b6b856453eb49218880723344b"
          }
        },
        "53bfc84e8a4c43f98095e6dcc543f85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7f76f9ecce49baa310f1acff5450a9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ac9aff5b622848f99528d8b1ef39d1f7",
            "value": "100%"
          }
        },
        "4ea0d00cccae46ebbe5043d5910667e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5604993df40848f49b4f39c2f9e3855c",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c169a1c70854b07bbcbe5fcbb508429",
            "value": 9
          }
        },
        "ca5c4ed971e94b4aa0dd8b8abc61363a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000f3fb5bce64e5980d53a64ec814d73",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ee17cdf174f440459ab0dfdd20e109b3",
            "value": " 9/9 [00:15&lt;00:00,  1.30s/it]"
          }
        },
        "985bf8b6b856453eb49218880723344b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7f76f9ecce49baa310f1acff5450a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac9aff5b622848f99528d8b1ef39d1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5604993df40848f49b4f39c2f9e3855c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c169a1c70854b07bbcbe5fcbb508429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "000f3fb5bce64e5980d53a64ec814d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee17cdf174f440459ab0dfdd20e109b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
